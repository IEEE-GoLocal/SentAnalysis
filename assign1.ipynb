{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "#Try to look for imdb dataset\n",
    "\n",
    "#Get hands-on with this dataset, get more info\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that below loaded data is already preprocessed.\n",
    "\n",
    "But in usual scenarios you will get tha dataset which contains text data. Following preprocessing techniques can be used: Try to google each of the below pre-processing techniques for more details\n",
    "\n",
    "uppercase letters to lower case\n",
    "english stopwords removal\n",
    "remove unwanted non alpha numeric characters\n",
    "remove punctuation marks\n",
    "and many more....\n",
    "\n",
    "Text Embeddings Convert text data to array(numerical form) using one of the following embedding techniques:\n",
    "\n",
    "Glove embedding(Global Vectors)\n",
    "Word2Vec\n",
    "BERT embeddings\n",
    "There are various other online and offline models for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train data - No need to preprocess and not that data is already embedded\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Why padding? zeros are added or existing sequences are truncated to ensure all sequences are of same lengths and they can be processed in atches.\n",
    "2.maxlen? Maxlen is the maximum length of a sequence after padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to go into details of each of the layers\n",
    "1.Just look at what is sequential, what does .add method do?-The Sequential model in Keras is a linear stack of layers. It allows for the creation of models layer-by-layer in a step-by-step fashion. Each layer added to the Sequential model is connected to the previous one.\n",
    "The .add method is used to add layers to the Sequential model\n",
    "\n",
    "2.What is dense layer and why to put that layer in end?- dense layer is a layer in which all of its neurons are connected to all neurons of the previous and next layers.It is put in the end to get a binary output.\n",
    "\n",
    "3.No need to go in details of a LSTM, knowing overview if it is sufficient-long short term memory.\n",
    "\n",
    "4.What is dropout?Dropout is a regularization technique used to prevent overfitting in neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))  # Adding another LSTM layer\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))  # Adding one more LSTM layer\n",
    "model.add(Dense(64, activation='relu'))  # Adding a Dense layer with a different activation function\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.What are optimzers? an optimizer is an algorithm or method used to update the parameters (weights and biases) of the model during the training process.\n",
    "\n",
    "2.What is confusion matrix-A confusion matrix is a table that is used to evaluate the performance of a classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Try to research about the shape (x,) and get the clear understanding of it-it shows a one dimensional array with x elements.\n",
    "\n",
    "2.Difference between (x,1) shape and (x,) shape-(x,1) shows a 2 dimnsional array with x rows and 1 column.\n",
    "\n",
    "Try to get handson with numpy library - very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "#reshaping\n",
    "y_train = y_train.reshape(25000,1)\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.epochs -epoch refers to one complete pass through the entire training dataset.\n",
    "\n",
    "2.python slicing - different techniques -Slicing is a technique used to extract a portion of a sequence\n",
    "\n",
    "3.training, testing, validation data\n",
    "Training Data: Used to train the model's parameters during the learning process.\n",
    "Testing Data: Used to evaluate the model's performance after training.\n",
    "Validation Data: A subset of the training data used to tune hyperparameters and prevent overfitting.\n",
    "\n",
    "4.What does below cell do?\n",
    "\n",
    "1.X_train[:2500, :] and y_train[:2500, :] represent the first 2500 samples of the training data and labels.\n",
    "2.batch_size=64: The number of samples processed in each batch during training.\n",
    "3.epochs=10: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "4.verbose=2: Displays training progress with additional information.\n",
    "5.validation_data=(X_test, y_test): Uses the testing dataset as validation data to monitor the model's performance during training.\n",
    "\n",
    "6.model.evaluate: Computes the loss and specified metrics on the testing data (X_test, y_test).\n",
    "7.score: Represents the computed loss.\n",
    "8.accuracy: Represents the computed accuracy.\n",
    "9.Model Prediction:\n",
    "\n",
    "model.predict(X_test[:5]): Generates predictions for the first 5 samples in the testing data (X_test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training\n",
    "\n",
    "model.fit(X_train[:2500,:], y_train[:2500,:],\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, y_test))\n",
    "#Predicting score, accuracy\n",
    "score, accuracy = model.evaluate(X_test, y_test, batch_size=32, verbose=2)\n",
    "#Predictions\n",
    "predictions = model.predict(X_test[:5])\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
