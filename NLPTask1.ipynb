{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KbG7IYK_-6lE"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "from keras.layers import LSTM, Dense, Embedding, GRU, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "#Try to look for imdb dataset\n",
    "\n",
    "#Get hands-on with this dataset, get more info\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that below loaded data is already preprocessed.\n",
    "\n",
    "But in usual scenarios you will get tha dataset which contains text data. Following preprocessing techniques can be used:\n",
    "Try to google each of the below pre-processing techniques for more details\n",
    "1. uppercase letters to lower case\n",
    "2. english stopwords removal\n",
    "3. remove unwanted non alpha numeric characters\n",
    "4. remove punctuation marks\n",
    "and many more....\n",
    "\n",
    "\n",
    "Text Embeddings\n",
    "Convert text data to array(numerical form) using one of the following embedding techniques:\n",
    "1. Glove embedding(Global Vectors)\n",
    "2. Word2Vec\n",
    "3. BERT embeddings\n",
    "There are various other online and offline models for embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xdzPuGaB-7UU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load train data - No need to preprocess and not that data is already embedded\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why padding?\n",
    "- we use padding to add layers of zeros outside the input matrix to preserve the spatial size we are working with\n",
    "2. maxlen?\n",
    "- maxlen is used to set the size of the sequence we want to use, in this case the size of the sentence of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0Je7pT5x--hq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No need to go into details of each of the layers\n",
    "2. Just look at what is sequential, what does .add method do?\n",
    "- it adds layers to the ML model architecture\n",
    "3. What is dense layer and why to put that layer in end?\n",
    "- dense layer here is our output layer that is fully interconnected with the previous layer in the model. we run the data through a activiation function in this layer\n",
    "4. No need to go in details of a LSTM, knowing overview if it is sufficient\n",
    "5. What is dropout?\n",
    "- if the model learns too much then the accuracy when testing with the test data decreases, so we can use dropouts to deactivate some neurons during training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to look for different model architectures that can be used and experiment with it.\n",
    "1. You can use multiple LSTM layer\n",
    "2. You can change the activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kGMCajRm_FZu"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are optimzers?\n",
    "- an optimizer adapts to the parameters and changes the parameters of the model like the learning rate and the weights to overall increase accuracy and reduce loss\n",
    "2. What is confusion matrix\n",
    "- it is a table that summarises the performance of a classification model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qYfB_Z5g_G4E"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to research about the shape (x,) and get the clear understanding of it\n",
    "2. Difference between (x,1) shape and (x,) shape\n",
    "- (x,1) would refer to an array of arrays with length 1. for example [[2] [3] [5] [9]]\n",
    "- (x,) would refer to a normal array\n",
    "3. Try to get handson with numpy library - very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping\n",
    "y_train = y_train.reshape(25000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the understanding of:\n",
    "1. epochs\n",
    "- number of times the data is passed through the model\n",
    "2. python slicing - different techniques\n",
    "3. training, testing, validation data\n",
    "4. What does below cell do?\n",
    "- the below cell is taking a set of 2500 samples of training and using 64 samples at a time to train through 10 iterations and comparing it with the test/validation data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Pm6piNqt_JN8",
    "outputId": "7ddec6c9-9781-4c3d-be98-e0d479f87ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "40/40 - 40s - loss: 0.6873 - accuracy: 0.5488 - val_loss: 0.6449 - val_accuracy: 0.6744 - 40s/epoch - 988ms/step\n",
      "Epoch 2/7\n",
      "40/40 - 25s - loss: 0.4103 - accuracy: 0.8348 - val_loss: 0.5911 - val_accuracy: 0.7244 - 25s/epoch - 635ms/step\n",
      "Epoch 3/7\n",
      "40/40 - 25s - loss: 0.1567 - accuracy: 0.9556 - val_loss: 0.6190 - val_accuracy: 0.7361 - 25s/epoch - 632ms/step\n",
      "Epoch 4/7\n",
      "40/40 - 26s - loss: 0.0580 - accuracy: 0.9892 - val_loss: 0.7887 - val_accuracy: 0.7411 - 26s/epoch - 641ms/step\n",
      "Epoch 5/7\n",
      "40/40 - 26s - loss: 0.0309 - accuracy: 0.9952 - val_loss: 0.8745 - val_accuracy: 0.7444 - 26s/epoch - 638ms/step\n",
      "Epoch 6/7\n",
      "40/40 - 25s - loss: 0.0157 - accuracy: 0.9988 - val_loss: 0.9541 - val_accuracy: 0.7429 - 25s/epoch - 628ms/step\n",
      "Epoch 7/7\n",
      "40/40 - 25s - loss: 0.0088 - accuracy: 0.9996 - val_loss: 1.0487 - val_accuracy: 0.7438 - 25s/epoch - 620ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d8ec63850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model training\n",
    "\n",
    "model.fit(X_train[:2500,:], y_train[:2500,:],\n",
    "          batch_size=64,\n",
    "          epochs=7,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "OcdqixhY_Lip",
    "outputId": "23deda5b-a597-4ffc-83fc-b486b837f1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 28s - loss: 1.0487 - accuracy: 0.7438 - 28s/epoch - 36ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predicting score, accuracy\n",
    "score, accuracy = model.evaluate(X_test, y_test, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gEZmR8g7_PH6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00629068],\n",
       "       [0.9944771 ],\n",
       "       [0.91207284],\n",
       "       [0.5075811 ],\n",
       "       [0.99468756]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predictions\n",
    "predictions = model.predict(X_test[:5])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
