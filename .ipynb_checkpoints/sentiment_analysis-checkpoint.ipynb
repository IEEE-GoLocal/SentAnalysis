{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "     ---------------------------------------- 0.0/83.6 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 71.7/83.6 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 83.6/83.6 kB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.28.2)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: bleach in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (6.0.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.2/78.2 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohith\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110696 sha256=d6ceff8efa8b2e39f2c97bb6a3016aa1a0ca436aaebec03f9f6a577f6d52e3a4\n",
      "  Stored in directory: c:\\users\\rohith\\appdata\\local\\pip\\cache\\wheels\\6a\\2b\\d0\\457dd27de499e9423caf738e743c4a3f82886ee6b19f89d5b7\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.5.16 python-slugify-8.0.1 text-unidecode-1.3 tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your kaggle api token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Rohith\\AppData\\Local\\Programs\\Python\\Python311\\Scripts\\kaggle.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"C:\\Users\\Rohith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"C:\\Users\\Rohith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py\", line 403, in authenticate\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
      "OSError: Could not find kaggle.json. Make sure it's located in C:\\Users\\Rohith\\.kaggle. Or use the environment method.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets list -s \"Amazon review\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/meetbanthia/.kaggle/kaggle.json'\n",
      "Downloading amazonreviews.zip to /Users/meetbanthia/Desktop/SentAnalysis\n",
      "100%|███████████████████████████████████████▉| 493M/493M [00:42<00:00, 12.8MB/s]\n",
      "100%|████████████████████████████████████████| 493M/493M [00:42<00:00, 12.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Download dataset\n",
    "!kaggle datasets download -d 'bittlingmayer/amazonreviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting zip file\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "file = './amazonreviews.zip'\n",
    "with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset')\n",
    "\n",
    "import os\n",
    "os.system(\"rm amazonreviews.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.ft.txt.bz2', 'train.ft.txt.bz2']\n"
     ]
    }
   ],
   "source": [
    "##Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, optimizers\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"./dataset\" directory.\n",
    "import os\n",
    "print(os.listdir(\"./dataset/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the text\n",
    "\n",
    "The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, so we have to convert that into a number and then take the rest to be the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "\n",
    "train_labels, train_texts = get_labels_and_texts('./dataset/train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('./dataset/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert to lowercase\n",
    "2. substitute non alphanumeric characters with whitespace\n",
    "3. Remove noascii characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\"\n",
    "    texts parameter is list of sentences which needs to be pre-processes\n",
    "    returns list containing each item of texts preprocessed\n",
    "    \"\"\"\n",
    "    preprocessed_texts = []\n",
    "\n",
    "    #issue1 - pre-processing\n",
    "\n",
    "    return preprocessed_texts\n",
    "        \n",
    "train_texts = preprocess(train_texts)\n",
    "test_texts = preprocess(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train data into train and validation data - try keeping test_size = 0.2\n",
    "# train_texts, val_texts, train_labels, val_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Tokenizer to convert text into model usable form\n",
    "\n",
    "#code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_texts ,val_texts ,test_texts now contains model usable form of texts\n",
    "\n",
    "#Do padding\n",
    "#find maxlen\n",
    "#Use padding to convert all vectors to maxlen and save them in train_texts ,val_texts ,test_texts itself\n",
    "\n",
    "#code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    #code here\n",
    "\n",
    "    #return model\n",
    "    \n",
    "lstm_model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "lstm_model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = lstm_model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_comment = \"Type here..\"\n",
    "\n",
    "\n",
    "lst = []\n",
    "lst.append(your_comment)\n",
    "preds = lstm_model.predict(lst)\n",
    "\n",
    "if preds[0]>0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell only if dataset is no longer needed\n",
    "\n",
    "#Deleting dataset as it exceeds 100 MB, github max limit\n",
    "import os \n",
    "os.system(\"rm -r dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
